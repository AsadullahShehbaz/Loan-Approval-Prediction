{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af050a92",
   "metadata": {},
   "source": [
    "#  Task 4: Loan Approval Prediction Description \n",
    "Description:\n",
    "- Dataset (Recommended): Loan-Approval-Prediction-Dataset (Kaggle\n",
    "- Build a model to predict whether a loan application will be approved\n",
    "- Handle missing values and encode categorical features\n",
    "- Train a classification model and evaluate performance on imbalanced data\n",
    "- Focus on precision, recall, and F1-score\n",
    "\n",
    "Tools & Libraries:\n",
    " - Python\n",
    " - Pandas\n",
    " \n",
    "Covered Topics\n",
    " - Scikit-learn\n",
    " - Binary classification |  Imbalanced data\n",
    "  \n",
    "Bonus:‚Ä®\n",
    "- Use SMOTE or other techniques to address class imbalance \n",
    "- Try logistic regression vs. decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52e8f5",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # rf model\n",
    "from sklearn.linear_model import LogisticRegression #lo model\n",
    "from sklearn.tree import DecisionTreeClassifier # dt model\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score , precision_score , recall_score , f1_score\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6007d",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a7af8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d88117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Missing value = {df.isnull().sum()}') # => missing value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb778959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Duplicated = {df.duplicated().sum()}') # =>duplicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94786d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89555538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show NULL value\n",
    "data_null = round(df.isna().sum() / df.shape[0] * 100, 2)\n",
    "data_null.to_frame(name = 'percent NULL data (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89160937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Distribution for Score \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df[' cibil_score'], bins=50, color='#B07AA1', edgecolor='white')\n",
    "plt.title(' cibil_score Distribution of Customers', fontsize=14, color='white')\n",
    "plt.xlabel(' cibil_score', color='white')\n",
    "plt.ylabel('Frequency', color='white')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a349f1c",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- This histogram depicts an approximately uniform distribution of data spanning from 300 to 900. \n",
    "- There is no discernible central tendency (like a mean or mode), as the frequencies across all bins are relatively consistent. \n",
    "- The data points are spread quite evenly, with most bins containing between 70 and 105 observations. \n",
    "- This pattern suggests that any value within the 300-900 range has a roughly equal probability of occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Target columns Analysis \n",
    "consistent_colors = [  '#B07AA1', '#FF9DA7']\n",
    "plt.figure(figsize=(10,6))\n",
    "explode = (0,0.03)\n",
    "plt.pie(df[' loan_status'].value_counts().values,\n",
    "        labels=df[' loan_status'].value_counts().index,\n",
    "        colors=consistent_colors[:len(df[' loan_status'].value_counts())],\n",
    "        explode=explode,\n",
    "        autopct=\"%1.2f%%\",\n",
    "        )\n",
    "plt.title('show Target ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0809e5",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- This pie chart illustrates an imbalanced dataset for the target variable. \n",
    "- The \"Approved\" class is the clear majority, accounting for 62.22% of the instances. \n",
    "- Conversely, the \"Rejected\" class represents the minority, making up the remaining 37.78%. \n",
    "- This class imbalance is a crucial consideration for predictive modeling, as it could lead to a model that is biased towards the majority \"Approved\" outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Set seaborn theme\n",
    "sns.set(style=\"whitegrid\", palette=\"Spectral\", font_scale=1.1)\n",
    "\n",
    "# Define color palette (vivid & unique)\n",
    "palette = sns.color_palette(\"Spectral\", len(numerical))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(numerical, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.boxplot(\n",
    "        x=df[col],\n",
    "        color=palette[i-1],\n",
    "        width=0.6,\n",
    "        fliersize=2,  \n",
    "        linewidth=1.2\n",
    "    )\n",
    "    plt.title(f\"Boxplot of {col}\", fontsize=11, fontweight=\"bold\", color=\"#333333\")\n",
    "    plt.xlabel(\"\")  \n",
    "plt.suptitle(\"Outlier Detection for Numerical Features\", fontsize=16, fontweight=\"bold\", color=\"#2c3e50\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # space for suptitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241c359",
   "metadata": {},
   "source": [
    "**Observations and Insights from Outlier Detection Plots**\n",
    "\n",
    "* **Significant Right-Skewness in Asset Features**: The boxplots for `residential_assets_value`, `commercial_assets_value`, `luxury_assets_value`, and `bank_asset_value` all reveal **highly right-skewed distributions**. This is indicated by the medians being close to the bottom of the boxes and numerous data points lying far beyond the upper whisker, highlighting a significant presence of **high-value outliers**. This suggests that while most applicants have modest asset values, a small subset possesses exceptionally high-value assets.\n",
    "\n",
    "* **Varied Distributions for Other Features**: The `cibil_score` distribution is **left-skewed**, implying that the dataset contains a higher concentration of individuals with good credit scores. In contrast, features like `income_annum` and `loan_term` appear relatively **symmetrical** with no obvious outliers. The `loan_amount` shows a slight right skew.\n",
    "\n",
    "* **Implications for Data Preprocessing**: The numerous outliers in the asset-related features can disproportionately influence the performance of many machine learning models. Therefore, it's crucial to apply appropriate data transformation techniques‚Äîsuch as **log transformation** to reduce the skewness or **capping/winsorization** to handle the extreme values‚Äîbefore proceeding with model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Distribution for all numerical columns \n",
    "palette =sns.color_palette(\"husl\",len(numerical))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "#for loop \n",
    "for i,col in enumerate(numerical,1):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(df[col],kde=True ,color=palette[i-1], bins=30)\n",
    "    plt.title(f'Distribution of {col} ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78132a",
   "metadata": {},
   "source": [
    "**Observations and Insights from Distribution Plots**\n",
    "* **Prevalence of Skewed Distributions**: These histograms confirm and provide more detail on the data's skewness. The distributions for all asset-related features (`residential`, `commercial`, `luxury`, `bank_asset_value`) and `loan_amount` are heavily **right-skewed**. This is evident from the concentration of data on the left side and a long tail extending to the right, which is typical for financial value data where most values are low and a few are exceptionally high. üí∞\n",
    "\n",
    "* **Confirmation of Other Distribution Shapes**: The plot for `cibil_score` clearly shows a **left-skewed distribution**, with a majority of applicants having higher scores. Conversely, `income_annum` exhibits a relatively **uniform distribution**, where different income levels within the range appear almost equally frequently. The `loan_id` is also uniformly distributed, as expected for an identifier.\n",
    "\n",
    "* **Discrete vs. Continuous Data**: The plots clearly distinguish between continuous and discrete features. For discrete variables like `no_of_dependents` and `loan_term`, the bars are more informative than the Kernel Density Estimate (KDE) curve, which can be misleading. The `no_of_dependents` follows a discrete uniform distribution.\n",
    "\n",
    "* **Modeling Implications**: The pronounced skew in key financial predictors reinforces the need for preprocessing. Applying transformations like a **logarithm or square root** to the right-skewed features is essential to normalize their distributions. This will help improve the performance and stability of many machine learning algorithms, especially linear models and those sensitive to feature scale. ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63872d",
   "metadata": {},
   "source": [
    "## Relation between Target column with Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4aabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "target = \"loan_status\"\n",
    "\n",
    "# Categorical\n",
    "categorical = [\"education\", \"self_employed\", \"no_of_dependents\"]\n",
    "for col in categorical:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=col, hue=target, data=df, palette=\"Set2\")\n",
    "    plt.title(f\"{col} vs {target}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d8bc6",
   "metadata": {},
   "source": [
    "\n",
    "üéì Education vs Loan Status\n",
    "- Loan approval rates are **almost identical** for Graduates (62.45%) and Non-Graduates (61.98%).  \n",
    "- Education level **does not appear to be a strong differentiator** in loan approval decisions.  \n",
    "- The dataset is **balanced** across both groups (‚âà50% each), ensuring fair comparison.\n",
    "\n",
    "---\n",
    "\n",
    "üíº Self Employed vs Loan Status\n",
    "- Approval rate is **virtually the same** for self-employed (62.23%) and non‚Äìself-employed (62.20%) applicants.  \n",
    "- Employment type **does not significantly influence** loan approval.  \n",
    "- The data is evenly distributed (‚âà50% Yes / No), which indicates no sampling bias.\n",
    "\n",
    "---\n",
    "\n",
    "üë®‚Äçüë©‚Äçüëß Number of Dependents vs Loan Status\n",
    "- Applicants with **no dependents** have the **highest approval rate** (64.19%), suggesting lower financial burden helps approval chances.  \n",
    "- As dependents increase, approval rate **slightly decreases** ‚Äî from 64.19% (0 dependents) to 60.33% (5 dependents).  \n",
    "- Majority of applicants fall within 0‚Äì4 dependents, showing a **stable but mild negative correlation** between dependents and approval.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Overall Insight:**  \n",
    "Loan approval seems **largely unaffected by education or employment type**, but a **slight decline with more dependents** indicates lenders may prefer applicants with fewer financial responsibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4552b52",
   "metadata": {},
   "source": [
    "## Relation between Target column with Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9069ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\n",
    "    \"income_annum\", \"loan_amount\", \"loan_term\", \"cibil_score\",\n",
    "    \"residential_assets_value\", \"commercial_assets_value\",\n",
    "    \"luxury_assets_value\", \"bank_asset_value\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, col in enumerate(numerical):\n",
    "    sns.boxplot(\n",
    "        data=df, \n",
    "        x=target, \n",
    "        y=col, \n",
    "        ax=axes[i], \n",
    "        palette=\"coolwarm\",\n",
    "        fliersize=2, \n",
    "        linewidth=1.2\n",
    "    )\n",
    "    axes[i].set_title(f\"{col} vs {target}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"\")  \n",
    "    axes[i].set_ylabel(col, fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Numerical Features vs Loan Status\", fontsize=16, fontweight=\"bold\", color=\"#2c3e50\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168dc1f",
   "metadata": {},
   "source": [
    "üìä **Numerical Features vs Loan Status**\n",
    "\n",
    "üí∞ Income vs Loan Status\n",
    "- The **median annual income ($\\text{income\\_annum}$) is slightly higher** for **Approved** loans compared to Rejected loans.\n",
    "- The **range and spread** of income are very similar for both groups, with Approved loans having slightly more high-end outliers.\n",
    "- Income is **not a sharp differentiator**, but higher income applicants show a marginal advantage.\n",
    "\n",
    "---\n",
    "\n",
    "üíµ Loan Amount vs Loan Status\n",
    "- The **median loan amount ($\\text{loan\\_amount}$) is lower** for **Approved** loans than for Rejected loans.\n",
    "- This suggests that lenders may be **more cautious** or approve smaller loan amounts more readily, potentially viewing larger requests as riskier.\n",
    "\n",
    "---\n",
    "\n",
    "‚è≥ Loan Term vs Loan Status\n",
    "- The **median loan term ($\\text{loan\\_term}$) is slightly shorter** for **Approved** loans than for Rejected loans.\n",
    "- This aligns with the loan amount insight, indicating a potential preference for **shorter-term, lower-risk** loans among approved applications.\n",
    "\n",
    "---\n",
    "\n",
    "üí≥ CIBIL Score vs Loan Status\n",
    "- The **median CIBIL score ($\\text{cibil\\_score}$) is substantially higher** for **Approved** loans (approx. 700) compared to Rejected loans (approx. 500).\n",
    "- This feature shows the **clearest separation**, with a high CIBIL score being a **critical factor** for loan approval.\n",
    "\n",
    "---\n",
    "\n",
    "üè† Asset Values vs Loan Status\n",
    "- The **median values** for **Residential, Commercial, and Luxury assets** are **uniformly lower** for **Approved** loans compared to Rejected loans.\n",
    "- This counter-intuitive trend suggests that applicants with **very high asset values** (often correlated with larger loan requests or higher financial complexity) might be **more likely to be rejected**, possibly due to applying for **disproportionately large loans** or having a different risk profile not captured here.\n",
    "- The **Bank Assets ($\\text{bank\\_asset\\_value}$)** distributions are **almost identical** for Approved and Rejected, suggesting they are **not a strong differentiator** in the decision.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Overall Insight:**\n",
    "**CIBIL Score is the strongest predictor** of loan status, with approved applicants having a significantly better score. While approved loans tend to be **smaller in amount and shorter in term**, high asset values surprisingly show a **mild negative correlation** with approval, possibly indicating rejection for complex or overly large loan applications from high-net-worth individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c7f17",
   "metadata": {},
   "source": [
    "# Outliers processor\n",
    "- We removed outliers to ensure the model learns from realistic applicant patterns instead of being influenced by extreme values. \n",
    "- Outliers can distort feature scales, reduce model accuracy, and cause biased predictions.\n",
    "- We chose the IQR (Interquartile Range) method because it‚Äôs simple, robust, and non-parametric‚Äîit doesn‚Äôt assume any specific data distribution. \n",
    "- This makes it ideal for financial data, where income or asset values often vary widely but shouldn‚Äôt dominate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b537de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers & Outliers processor before split data\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    \n",
    "    df[column] = df[column].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "\n",
    "num_cols = ['no_of_dependents','income_annum','loan_amount','loan_term',\n",
    "            'cibil_score','residential_assets_value',\n",
    "            'commercial_assets_value','luxury_assets_value','bank_asset_value']\n",
    "\n",
    "for col in num_cols:\n",
    "    df = remove_outliers_iqr(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete unneeded column\n",
    "df = df.drop(columns=['loan_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c35a2",
   "metadata": {},
   "source": [
    "# Encoding Categorical columns\n",
    "\n",
    "- We used **Label Encoding** to convert categorical variables like `education`, `self_employed`, and `loan_status` into numerical form for model training.  \n",
    "- Machine learning algorithms **cannot process text labels directly**, so encoding ensures they can interpret categories as numeric values.  \n",
    "- We chose **Label Encoding** because these features have **only two categories (binary)** ‚Äî making it a **simple, efficient, and memory-friendly** method compared to One-Hot Encoding.  \n",
    "- It helps keep the dataset compact and ready for ML models that expect numerical input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to categorical\n",
    "le = LabelEncoder()\n",
    "df['education'] = le.fit_transform(df['education'])\n",
    "df['self_employed'] = le.fit_transform(df['self_employed'])\n",
    "df['loan_status'] = le.fit_transform(df['loan_status'])  # => target \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bb3d9",
   "metadata": {},
   "source": [
    "# split data to Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['loan_status'])\n",
    "y = df['loan_status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d6a03",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18d1ce",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28650887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# üìä Evaluate model performance\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision : {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall    : {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score  : {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# üîç Detailed metrics per class\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249745cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Model Performance:**\n",
    "- ‚úÖ **Accuracy:** 98.2% ‚Äî predicts loan approvals very well.\n",
    "- üéØ **Precision & Recall:** High for both approved and rejected loans. Recall for approved loans is slightly lower (0.96), meaning a few approvals are missed but false approvals are rare.\n",
    "- ‚öñÔ∏è **F1-Score:** 0.98‚Äì0.99, indicating balanced performance across classes.\n",
    "\n",
    "**Business Insight:**\n",
    "- The model is **conservative**, minimizing risky loan approvals.\n",
    "\n",
    "**Next Steps:**\n",
    "- Analyze misclassified approvals (false negatives) to further improve recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Loan Approval Model')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance as a DataFrame\n",
    "feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
    "feat_imp = feat_imp.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp, palette='viridis')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f8b42",
   "metadata": {},
   "source": [
    "üå≤ **Random Forest Feature Importance Insights**\n",
    "\n",
    "- ü•á **Top Predictor:** `cibil_score` dominates the model, contributing ~80% of total importance.  \n",
    "- üîπ **Moderate Predictors:** `loan_term` and `loan_amount` have some impact but are far behind `cibil_score`.  \n",
    "- üìâ **Low-Impact Features:** Asset-related features (`luxury_assets_value`, `commercial_assets_value`, etc.) and categorical features (`self_employed`, `education`) contribute very little.  \n",
    "- ‚úÖ **Key Insight:** Model decisions rely heavily on `cibil_score`; other features have minimal effect, suggesting opportunities for **feature selection** or model simplification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407bf0f",
   "metadata": {},
   "source": [
    "# Class Imbalance\n",
    "- ‚öñÔ∏è **Balance Classes:** When one class dominates (e.g., approved loans), the model may become biased toward it. SMOTE balances the dataset by generating synthetic samples for the minority class.  \n",
    "- üîÑ **Improve Model Performance:** Balancing helps the model better learn patterns of the minority class, improving recall and F1-score.  \n",
    "- üß™ **Prevent Data Bias:** Using SMOTE on training data only avoids leakage and ensures fair learning.  \n",
    "- üßê **Better Predictions:** After resampling, the model can predict both approved and rejected loans more accurately.  \n",
    "- ‚úÖ **Reproducibility:** Setting `random_state` ensures consistent results across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e934e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# üéØ Tackling Imbalanced Data:\n",
    "# When one class (e.g., 'Approved' loans) has many more samples than the other ('Rejected' loans), \n",
    "# our model might become biased and struggle to predict the minority class correctly.\n",
    "\n",
    "# üß™ Initialize the SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# SMOTE creates *synthetic* new samples for the minority class to balance the dataset.\n",
    "# setting random_state ensures we get the same results every time (reproducibility!).\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# üîÑ Apply SMOTE to the training data\n",
    "# We fit and resample the data in one step. SMOTE only uses the training data \n",
    "# (X_train, y_train) to prevent data leakage from the test set.\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# üßê Check the results (the crucial step!)\n",
    "# Before SMOTE: We see the original imbalance (e.g., more 1s than 0s).\n",
    "print(\"Before SMOTE:\", y_train.value_counts().to_dict())\n",
    "\n",
    "# After SMOTE: The classes are now perfectly balanced! (e.g., equal numbers of 1s and 0s).\n",
    "# This balanced data (X_resampled, y_resampled) will be used to train our final model.\n",
    "print(\"After SMOTE:\", y_resampled.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc1a43",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4352f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c72010",
   "metadata": {},
   "source": [
    "üîç Model Comparison: Logistic Regression vs Random Forest\n",
    "\n",
    "| Metric                | Logistic Regression (After SMOTE) | Random Forest |\n",
    "|-----------------------|---------------------------------|---------------|\n",
    "| **Accuracy**          | 81%                              | 98.2%         |\n",
    "| **Precision (Approved)** | 0.83                            | 0.99          |\n",
    "| **Recall (Approved)**    | 0.63                            | 0.96          |\n",
    "| **F1-Score (Approved)**  | 0.71                            | 0.98          |\n",
    "| **Precision & Recall (Rejected)** | 0.80 / 0.92              | 0.98 / 0.99   |\n",
    "\n",
    "### üîπ Insights\n",
    "- **Random Forest clearly outperforms Logistic Regression** across all metrics, especially for predicting approved loans.  \n",
    "- Logistic Regression struggles with **recall for approved loans** (0.63), even after SMOTE, whereas Random Forest maintains high recall (0.96).  \n",
    "- **Business Impact:** RF is safer for loan decisions, minimizing risky false approvals while accurately identifying approvals.  \n",
    "- **Next Steps:** Logistic Regression may still be useful for interpretability, but for best predictive performance, Random Forest is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87576954",
   "metadata": {},
   "source": [
    "# üéØ Conclusion & Key Takeaways\n",
    "\n",
    "- ‚úÖ **Random Forest is the best performer** for this dataset, achieving 98.2% accuracy and strong precision & recall across both approved and rejected loans.  \n",
    "- üí° **Feature Insights:** `cibil_score` is the dominant predictor, while loan amount and term contribute moderately; demographic and asset-related features have minimal impact.  \n",
    "- ‚öñÔ∏è **Imbalanced Data Handling:** Applying SMOTE improved model learning for minority class (approved loans), especially for Logistic Regression, though Random Forest still outperforms.  \n",
    "- üîç **Business Perspective:** The model is conservative in approving loans, minimizing risk and supporting data-driven decision-making.  \n",
    "- üöÄ **Next Steps:** Further improvements could include hyperparameter tuning, exploring ensemble methods, or adding new features for even better predictive performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
