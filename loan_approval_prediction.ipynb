{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af050a92",
   "metadata": {},
   "source": [
    "#  Task 4: Loan Approval Prediction Description \n",
    "Description:\n",
    "- Dataset (Recommended): Loan-Approval-Prediction-Dataset (Kaggle\n",
    "- Build a model to predict whether a loan application will be approved\n",
    "- Handle missing values and encode categorical features\n",
    "- Train a classification model and evaluate performance on imbalanced data\n",
    "- Focus on precision, recall, and F1-score\n",
    "\n",
    "Tools & Libraries:\n",
    " - Python\n",
    " - Pandas\n",
    " \n",
    "Covered Topics\n",
    " - Scikit-learn\n",
    " - Binary classification |  Imbalanced data\n",
    "  \n",
    "Bonus: \n",
    "- Use SMOTE or other techniques to address class imbalance \n",
    "- Try logistic regression vs. decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52e8f5",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # rf model\n",
    "from sklearn.linear_model import LogisticRegression #lo model\n",
    "from sklearn.tree import DecisionTreeClassifier # dt model\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score , precision_score , recall_score , f1_score\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6007d",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a7af8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d88117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Missing value = {df.isnull().sum()}') # => missing value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb778959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Duplicated = {df.duplicated().sum()}') # =>duplicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94786d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89555538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show NULL value\n",
    "data_null = round(df.isna().sum() / df.shape[0] * 100, 2)\n",
    "data_null.to_frame(name = 'percent NULL data (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89160937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Distribution for Score \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df[' cibil_score'], bins=50, color='#B07AA1', edgecolor='white')\n",
    "plt.title(' cibil_score Distribution of Customers', fontsize=14, color='white')\n",
    "plt.xlabel(' cibil_score', color='white')\n",
    "plt.ylabel('Frequency', color='white')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a349f1c",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- This histogram depicts an approximately uniform distribution of data spanning from 300 to 900. \n",
    "- There is no discernible central tendency (like a mean or mode), as the frequencies across all bins are relatively consistent. \n",
    "- The data points are spread quite evenly, with most bins containing between 70 and 105 observations. \n",
    "- This pattern suggests that any value within the 300-900 range has a roughly equal probability of occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Target columns Analysis \n",
    "consistent_colors = [  '#B07AA1', '#FF9DA7']\n",
    "plt.figure(figsize=(10,6))\n",
    "explode = (0,0.03)\n",
    "plt.pie(df[' loan_status'].value_counts().values,\n",
    "        labels=df[' loan_status'].value_counts().index,\n",
    "        colors=consistent_colors[:len(df[' loan_status'].value_counts())],\n",
    "        explode=explode,\n",
    "        autopct=\"%1.2f%%\",\n",
    "        )\n",
    "plt.title('show Target ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0809e5",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- This pie chart illustrates an imbalanced dataset for the target variable. \n",
    "- The \"Approved\" class is the clear majority, accounting for 62.22% of the instances. \n",
    "- Conversely, the \"Rejected\" class represents the minority, making up the remaining 37.78%. \n",
    "- This class imbalance is a crucial consideration for predictive modeling, as it could lead to a model that is biased towards the majority \"Approved\" outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Set seaborn theme\n",
    "sns.set(style=\"whitegrid\", palette=\"Spectral\", font_scale=1.1)\n",
    "\n",
    "# Define color palette (vivid & unique)\n",
    "palette = sns.color_palette(\"Spectral\", len(numerical))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, col in enumerate(numerical, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.boxplot(\n",
    "        x=df[col],\n",
    "        color=palette[i-1],\n",
    "        width=0.6,\n",
    "        fliersize=2,  \n",
    "        linewidth=1.2\n",
    "    )\n",
    "    plt.title(f\"Boxplot of {col}\", fontsize=11, fontweight=\"bold\", color=\"#333333\")\n",
    "    plt.xlabel(\"\")  \n",
    "plt.suptitle(\"Outlier Detection for Numerical Features\", fontsize=16, fontweight=\"bold\", color=\"#2c3e50\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # space for suptitle\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241c359",
   "metadata": {},
   "source": [
    "**Observations and Insights from Outlier Detection Plots**\n",
    "\n",
    "* **Significant Right-Skewness in Asset Features**: The boxplots for `residential_assets_value`, `commercial_assets_value`, `luxury_assets_value`, and `bank_asset_value` all reveal **highly right-skewed distributions**. This is indicated by the medians being close to the bottom of the boxes and numerous data points lying far beyond the upper whisker, highlighting a significant presence of **high-value outliers**. This suggests that while most applicants have modest asset values, a small subset possesses exceptionally high-value assets.\n",
    "\n",
    "* **Varied Distributions for Other Features**: The `cibil_score` distribution is **left-skewed**, implying that the dataset contains a higher concentration of individuals with good credit scores. In contrast, features like `income_annum` and `loan_term` appear relatively **symmetrical** with no obvious outliers. The `loan_amount` shows a slight right skew.\n",
    "\n",
    "* **Implications for Data Preprocessing**: The numerous outliers in the asset-related features can disproportionately influence the performance of many machine learning models. Therefore, it's crucial to apply appropriate data transformation techniques—such as **log transformation** to reduce the skewness or **capping/winsorization** to handle the extreme values—before proceeding with model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Distribution for all numerical columns \n",
    "palette =sns.color_palette(\"husl\",len(numerical))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "#for loop \n",
    "for i,col in enumerate(numerical,1):\n",
    "    plt.subplot(4,3,i)\n",
    "    sns.histplot(df[col],kde=True ,color=palette[i-1], bins=30)\n",
    "    plt.title(f'Distribution of {col} ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78132a",
   "metadata": {},
   "source": [
    "**Observations and Insights from Distribution Plots**\n",
    "* **Prevalence of Skewed Distributions**: These histograms confirm and provide more detail on the data's skewness. The distributions for all asset-related features (`residential`, `commercial`, `luxury`, `bank_asset_value`) and `loan_amount` are heavily **right-skewed**. This is evident from the concentration of data on the left side and a long tail extending to the right, which is typical for financial value data where most values are low and a few are exceptionally high. 💰\n",
    "\n",
    "* **Confirmation of Other Distribution Shapes**: The plot for `cibil_score` clearly shows a **left-skewed distribution**, with a majority of applicants having higher scores. Conversely, `income_annum` exhibits a relatively **uniform distribution**, where different income levels within the range appear almost equally frequently. The `loan_id` is also uniformly distributed, as expected for an identifier.\n",
    "\n",
    "* **Discrete vs. Continuous Data**: The plots clearly distinguish between continuous and discrete features. For discrete variables like `no_of_dependents` and `loan_term`, the bars are more informative than the Kernel Density Estimate (KDE) curve, which can be misleading. The `no_of_dependents` follows a discrete uniform distribution.\n",
    "\n",
    "* **Modeling Implications**: The pronounced skew in key financial predictors reinforces the need for preprocessing. Applying transformations like a **logarithm or square root** to the right-skewed features is essential to normalize their distributions. This will help improve the performance and stability of many machine learning algorithms, especially linear models and those sensitive to feature scale. ⚙️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63872d",
   "metadata": {},
   "source": [
    "## Relation between Target column with Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4aabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "target = \"loan_status\"\n",
    "\n",
    "# Categorical\n",
    "categorical = [\"education\", \"self_employed\", \"no_of_dependents\"]\n",
    "for col in categorical:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=col, hue=target, data=df, palette=\"Set2\")\n",
    "    plt.title(f\"{col} vs {target}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d8bc6",
   "metadata": {},
   "source": [
    "\n",
    "🎓 Education vs Loan Status\n",
    "- Loan approval rates are **almost identical** for Graduates (62.45%) and Non-Graduates (61.98%).  \n",
    "- Education level **does not appear to be a strong differentiator** in loan approval decisions.  \n",
    "- The dataset is **balanced** across both groups (≈50% each), ensuring fair comparison.\n",
    "\n",
    "---\n",
    "\n",
    "💼 Self Employed vs Loan Status\n",
    "- Approval rate is **virtually the same** for self-employed (62.23%) and non–self-employed (62.20%) applicants.  \n",
    "- Employment type **does not significantly influence** loan approval.  \n",
    "- The data is evenly distributed (≈50% Yes / No), which indicates no sampling bias.\n",
    "\n",
    "---\n",
    "\n",
    "👨‍👩‍👧 Number of Dependents vs Loan Status\n",
    "- Applicants with **no dependents** have the **highest approval rate** (64.19%), suggesting lower financial burden helps approval chances.  \n",
    "- As dependents increase, approval rate **slightly decreases** — from 64.19% (0 dependents) to 60.33% (5 dependents).  \n",
    "- Majority of applicants fall within 0–4 dependents, showing a **stable but mild negative correlation** between dependents and approval.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Overall Insight:**  \n",
    "Loan approval seems **largely unaffected by education or employment type**, but a **slight decline with more dependents** indicates lenders may prefer applicants with fewer financial responsibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4552b52",
   "metadata": {},
   "source": [
    "## Relation between Target column with Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9069ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [\n",
    "    \"income_annum\", \"loan_amount\", \"loan_term\", \"cibil_score\",\n",
    "    \"residential_assets_value\", \"commercial_assets_value\",\n",
    "    \"luxury_assets_value\", \"bank_asset_value\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, col in enumerate(numerical):\n",
    "    sns.boxplot(\n",
    "        data=df, \n",
    "        x=target, \n",
    "        y=col, \n",
    "        ax=axes[i], \n",
    "        palette=\"coolwarm\",\n",
    "        fliersize=2, \n",
    "        linewidth=1.2\n",
    "    )\n",
    "    axes[i].set_title(f\"{col} vs {target}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"\")  \n",
    "    axes[i].set_ylabel(col, fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Numerical Features vs Loan Status\", fontsize=16, fontweight=\"bold\", color=\"#2c3e50\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168dc1f",
   "metadata": {},
   "source": [
    "📊 **Numerical Features vs Loan Status**\n",
    "\n",
    "💰 Income vs Loan Status\n",
    "- The **median annual income ($\\text{income\\_annum}$) is slightly higher** for **Approved** loans compared to Rejected loans.\n",
    "- The **range and spread** of income are very similar for both groups, with Approved loans having slightly more high-end outliers.\n",
    "- Income is **not a sharp differentiator**, but higher income applicants show a marginal advantage.\n",
    "\n",
    "---\n",
    "\n",
    "💵 Loan Amount vs Loan Status\n",
    "- The **median loan amount ($\\text{loan\\_amount}$) is lower** for **Approved** loans than for Rejected loans.\n",
    "- This suggests that lenders may be **more cautious** or approve smaller loan amounts more readily, potentially viewing larger requests as riskier.\n",
    "\n",
    "---\n",
    "\n",
    "⏳ Loan Term vs Loan Status\n",
    "- The **median loan term ($\\text{loan\\_term}$) is slightly shorter** for **Approved** loans than for Rejected loans.\n",
    "- This aligns with the loan amount insight, indicating a potential preference for **shorter-term, lower-risk** loans among approved applications.\n",
    "\n",
    "---\n",
    "\n",
    "💳 CIBIL Score vs Loan Status\n",
    "- The **median CIBIL score ($\\text{cibil\\_score}$) is substantially higher** for **Approved** loans (approx. 700) compared to Rejected loans (approx. 500).\n",
    "- This feature shows the **clearest separation**, with a high CIBIL score being a **critical factor** for loan approval.\n",
    "\n",
    "---\n",
    "\n",
    "🏠 Asset Values vs Loan Status\n",
    "- The **median values** for **Residential, Commercial, and Luxury assets** are **uniformly lower** for **Approved** loans compared to Rejected loans.\n",
    "- This counter-intuitive trend suggests that applicants with **very high asset values** (often correlated with larger loan requests or higher financial complexity) might be **more likely to be rejected**, possibly due to applying for **disproportionately large loans** or having a different risk profile not captured here.\n",
    "- The **Bank Assets ($\\text{bank\\_asset\\_value}$)** distributions are **almost identical** for Approved and Rejected, suggesting they are **not a strong differentiator** in the decision.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Overall Insight:**\n",
    "**CIBIL Score is the strongest predictor** of loan status, with approved applicants having a significantly better score. While approved loans tend to be **smaller in amount and shorter in term**, high asset values surprisingly show a **mild negative correlation** with approval, possibly indicating rejection for complex or overly large loan applications from high-net-worth individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c7f17",
   "metadata": {},
   "source": [
    "# Outliers processor\n",
    "- We removed outliers to ensure the model learns from realistic applicant patterns instead of being influenced by extreme values. \n",
    "- Outliers can distort feature scales, reduce model accuracy, and cause biased predictions.\n",
    "- We chose the IQR (Interquartile Range) method because it’s simple, robust, and non-parametric—it doesn’t assume any specific data distribution. \n",
    "- This makes it ideal for financial data, where income or asset values often vary widely but shouldn’t dominate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b537de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers & Outliers processor before split data\n",
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    \n",
    "    df[column] = df[column].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "\n",
    "num_cols = ['no_of_dependents','income_annum','loan_amount','loan_term',\n",
    "            'cibil_score','residential_assets_value',\n",
    "            'commercial_assets_value','luxury_assets_value','bank_asset_value']\n",
    "\n",
    "for col in num_cols:\n",
    "    df = remove_outliers_iqr(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete unneeded column\n",
    "df = df.drop(columns=['loan_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c35a2",
   "metadata": {},
   "source": [
    "# Encoding Categorical columns\n",
    "\n",
    "- We used **Label Encoding** to convert categorical variables like `education`, `self_employed`, and `loan_status` into numerical form for model training.  \n",
    "- Machine learning algorithms **cannot process text labels directly**, so encoding ensures they can interpret categories as numeric values.  \n",
    "- We chose **Label Encoding** because these features have **only two categories (binary)** — making it a **simple, efficient, and memory-friendly** method compared to One-Hot Encoding.  \n",
    "- It helps keep the dataset compact and ready for ML models that expect numerical input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to categorical\n",
    "le = LabelEncoder()\n",
    "df['education'] = le.fit_transform(df['education'])\n",
    "df['self_employed'] = le.fit_transform(df['self_employed'])\n",
    "df['loan_status'] = le.fit_transform(df['loan_status'])  # => target \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bb3d9",
   "metadata": {},
   "source": [
    "# split data to Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['loan_status'])\n",
    "y = df['loan_status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d6a03",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18d1ce",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28650887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 📊 Evaluate model performance\n",
    "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision : {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall    : {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score  : {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# 🔍 Detailed metrics per class\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249745cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Model Performance:**\n",
    "- ✅ **Accuracy:** 98.2% — predicts loan approvals very well.\n",
    "- 🎯 **Precision & Recall:** High for both approved and rejected loans. Recall for approved loans is slightly lower (0.96), meaning a few approvals are missed but false approvals are rare.\n",
    "- ⚖️ **F1-Score:** 0.98–0.99, indicating balanced performance across classes.\n",
    "\n",
    "**Business Insight:**\n",
    "- The model is **conservative**, minimizing risky loan approvals.\n",
    "\n",
    "**Next Steps:**\n",
    "- Analyze misclassified approvals (false negatives) to further improve recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Loan Approval Model')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance as a DataFrame\n",
    "feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})\n",
    "feat_imp = feat_imp.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_imp, palette='viridis')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f8b42",
   "metadata": {},
   "source": [
    "🌲 **Random Forest Feature Importance Insights**\n",
    "\n",
    "- 🥇 **Top Predictor:** `cibil_score` dominates the model, contributing ~80% of total importance.  \n",
    "- 🔹 **Moderate Predictors:** `loan_term` and `loan_amount` have some impact but are far behind `cibil_score`.  \n",
    "- 📉 **Low-Impact Features:** Asset-related features (`luxury_assets_value`, `commercial_assets_value`, etc.) and categorical features (`self_employed`, `education`) contribute very little.  \n",
    "- ✅ **Key Insight:** Model decisions rely heavily on `cibil_score`; other features have minimal effect, suggesting opportunities for **feature selection** or model simplification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407bf0f",
   "metadata": {},
   "source": [
    "# Class Imbalance\n",
    "- ⚖️ **Balance Classes:** When one class dominates (e.g., approved loans), the model may become biased toward it. SMOTE balances the dataset by generating synthetic samples for the minority class.  \n",
    "- 🔄 **Improve Model Performance:** Balancing helps the model better learn patterns of the minority class, improving recall and F1-score.  \n",
    "- 🧪 **Prevent Data Bias:** Using SMOTE on training data only avoids leakage and ensures fair learning.  \n",
    "- 🧐 **Better Predictions:** After resampling, the model can predict both approved and rejected loans more accurately.  \n",
    "- ✅ **Reproducibility:** Setting `random_state` ensures consistent results across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e934e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 🎯 Tackling Imbalanced Data:\n",
    "# When one class (e.g., 'Approved' loans) has many more samples than the other ('Rejected' loans), \n",
    "# our model might become biased and struggle to predict the minority class correctly.\n",
    "\n",
    "# 🧪 Initialize the SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# SMOTE creates *synthetic* new samples for the minority class to balance the dataset.\n",
    "# setting random_state ensures we get the same results every time (reproducibility!).\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# 🔄 Apply SMOTE to the training data\n",
    "# We fit and resample the data in one step. SMOTE only uses the training data \n",
    "# (X_train, y_train) to prevent data leakage from the test set.\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# 🧐 Check the results (the crucial step!)\n",
    "# Before SMOTE: We see the original imbalance (e.g., more 1s than 0s).\n",
    "print(\"Before SMOTE:\", y_train.value_counts().to_dict())\n",
    "\n",
    "# After SMOTE: The classes are now perfectly balanced! (e.g., equal numbers of 1s and 0s).\n",
    "# This balanced data (X_resampled, y_resampled) will be used to train our final model.\n",
    "print(\"After SMOTE:\", y_resampled.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc1a43",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4352f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
    "print(classification_report(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c72010",
   "metadata": {},
   "source": [
    "🔍 Model Comparison: Logistic Regression vs Random Forest\n",
    "\n",
    "| Metric                | Logistic Regression (After SMOTE) | Random Forest |\n",
    "|-----------------------|---------------------------------|---------------|\n",
    "| **Accuracy**          | 81%                              | 98.2%         |\n",
    "| **Precision (Approved)** | 0.83                            | 0.99          |\n",
    "| **Recall (Approved)**    | 0.63                            | 0.96          |\n",
    "| **F1-Score (Approved)**  | 0.71                            | 0.98          |\n",
    "| **Precision & Recall (Rejected)** | 0.80 / 0.92              | 0.98 / 0.99   |\n",
    "\n",
    "### 🔹 Insights\n",
    "- **Random Forest clearly outperforms Logistic Regression** across all metrics, especially for predicting approved loans.  \n",
    "- Logistic Regression struggles with **recall for approved loans** (0.63), even after SMOTE, whereas Random Forest maintains high recall (0.96).  \n",
    "- **Business Impact:** RF is safer for loan decisions, minimizing risky false approvals while accurately identifying approvals.  \n",
    "- **Next Steps:** Logistic Regression may still be useful for interpretability, but for best predictive performance, Random Forest is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87576954",
   "metadata": {},
   "source": [
    "# 🎯 Conclusion & Key Takeaways\n",
    "\n",
    "- ✅ **Random Forest is the best performer** for this dataset, achieving 98.2% accuracy and strong precision & recall across both approved and rejected loans.  \n",
    "- 💡 **Feature Insights:** `cibil_score` is the dominant predictor, while loan amount and term contribute moderately; demographic and asset-related features have minimal impact.  \n",
    "- ⚖️ **Imbalanced Data Handling:** Applying SMOTE improved model learning for minority class (approved loans), especially for Logistic Regression, though Random Forest still outperforms.  \n",
    "- 🔍 **Business Perspective:** The model is conservative in approving loans, minimizing risk and supporting data-driven decision-making.  \n",
    "- 🚀 **Next Steps:** Further improvements could include hyperparameter tuning, exploring ensemble methods, or adding new features for even better predictive performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
